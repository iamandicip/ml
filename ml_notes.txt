Supervised learning algorithms:
	Linear regression
	Logistic regression
	Neural networks
	SVM with linear kernel
	SVM with Gaussian kernel

SVM parameters (C)
Bias and variance trade off
Must chose C
C plays a role similar to 1/LAMBDA (where LAMBDA is the regularization parameter)
Large C gives a hypothesis of low bias high variance --> overfitting
Small C gives a hypothesis of high bias low variance --> under-fitting
SVM parameters (σ2)
Parameter for calculating f values
Large σ2 - f features vary more smoothly - higher bias, lower variance
Small σ2 - f features vary abruptly - low bias, high variance

Logistic regression vs. SVM
When should you use SVM and when is logistic regression more applicable
If n (features) is large vs. m (training set)
e.g. text classification problem
Feature vector dimension is 10 000
Training set is 10 - 1000
Then use logistic regression or SVM with a linear kernel
If n is small and m is intermediate
n = 1 - 1000
m = 10 - 10 000
Gaussian kernel is good
If n is small and m is large
n = 1 - 1000
m = 50 000+
SVM will be slow to run with Gaussian kernel
In that case
Manually create or add more features
Use logistic regression of SVM with a linear kernel
Logistic regression and SVM with a linear kernel are pretty similar
Do similar things
Get similar performance
A lot of SVM's power is using different kernels to learn complex non-linear functions
For all these regimes a well designed NN should work
But, for some of these problems a NN might be slower - SVM well implemented would be faster
SVM has a convex optimization problem - so you get a global minimum
It's not always clear how to chose an algorithm
Often more important to get enough data
Designing new features
Debugging the algorithm
SVM is widely perceived a very powerful learning algorithm

——————————————————————————————————————————————————————————

Unsupervised learning algorithms
	K-means -> clustering
	PCA (Principal Component Analysis) - for dimensionality reduction and compression

Always perform mean normalization and feature scaling for PCA
PCA is not linear regression because:
	in PCA the error is calculated as the sum of squares of the projections
	in linear regression the error is calculated as the distance parallel with the axis
	linear regression tries to predict the value of y
	PCA tries to find other features -> a lower dimensional surface onto which to project x

Use PCA only on the training set, not on the validation/test set!

Don't use PCA to prevent overfitting!
Because it doesn't take into account the values of y so it might throw away valuable data.
For preventing overfitting, user regularization instead.

Before implementing PCA, try training with original data.

——————————————————————————————————————————————————————————
Anomaly detection vs supervised learning

Anomaly detection
- small number of positives examples vs large number of negative examples
- many different types of anomalies -> hard to learn how all the anomalies look like
- usages: fraud detection, manufacturing, monitoring machines, etc

Supervised learning
- large number of both positive and negative examples
- enough positive examples to learn how they are like
- spam classification, weather prediction, cancer classification

You can apply different transformation to the features to make it look more gaussian:
log(x), sqrt(x), log(x + c), x^(1/3), exp(x) etc

——————————————————————————————————————————————————————————

Multivariate Gaussian distribution when some of the features are correlated.
It is a generalization of the original Gaussian distribution.
If m < n or the features are redundant or liniarly dependent, the matrix inversion will fail

When to apply Original Gaussian model vs Multivariate Gaussian model?

Original Gaussian model
- used more often
- when the features are created manually
- computationally cheaper for large number of features
- works ok even for small number of training set

Multivariate Gaussian model
- when we want to automatically capture correlations between features
- expensive to compute due to the inverse of Sigma nxn
- must have m >> n (m > 10n)

——————————————————————————————————————————————————————————
Anomaly detection -> gaussian distribution

Collaborative filtering - for recommender systems
——————————————————————————————————————————————————————————

Stochastic gradient descent - for very large data sets
doesn’t converge to a minimum
it is recommmended to shuffle the data before starting

Stochastic gradient descent uses a single example at a time in each iteration
Batch gradient descent uses all m examples in each iteration
Mini-batch gradient descent uses b examples in each iteration	
b in [2, 100]

mini-batch can be faster than stochastic because of vectorization -> parralelization

check for convergence by plotting the cost function every let’s say 1000 iterations
if the cost function is increasing, maybe alpha is too big.

usually, alpha is held constant

to make stochastic gradient converge to the minimum, slowly decrease alpha every iteration
alpha = c1 / (iterations + c2)

——————————————————————————————————————————————————————————
 
Online learning
doesn’t store the training data, it just updates theta
it can adapt to changing user preferences
Recommending products to users

——————————————————————————————————————————————————————————

MapReduce - split the training set into multiple parts and send them to other machines/cores to do the partial sum calculation
get the partial results from the machines and use them in the final calculation

——————————————————————————————————————————————————————————

You can get more training data by generating artificial data synthesis or synthesize data by introducing distorsions.

Apply ceiling analysis on deciding what part of the machine learning pipeline should be improved.


